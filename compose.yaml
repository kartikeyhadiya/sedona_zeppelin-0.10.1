version: '3.9'
services:
  spark-master:
    image: bde2020/spark-master:3.0.1-hadoop3.2
    container_name: spark-master
    ports:
      - "8080:8080"
      - "7077:7077"
    environment:
      - INIT_DAEMON_STEP=setup_spark
    volumes:
      - spark_volume:/spark
      - ./data:/usr/data:rw
  spark-worker-1:
    image: bde2020/spark-worker:3.0.1-hadoop3.2
    container_name: spark-worker-1
    depends_on:
      - spark-master
    ports:
      - "8081:8081"
    volumes:
      - spark_volume:/spark
      - ./data:/usr/data:rw
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
      - "SPARK_WORKER_CORES=2"
      - "SPARK_WORKER_MEMORY=4g"
  zeppelin:
    image: kartikeyhadiya/sedona_zeppelin-0.10.1:1.3.1-incubating
    build: 
      context: ./
      dockerfile: Dockerfile
    user: "1000:1000"
    container_name: zeppelin
    depends_on:
      - spark-master
      - spark-worker-1
    ports:
      - "8082:8080"
    volumes:
      - ./zeppelin/notebook:/opt/zeppelin/notebook
      - ./zeppelin/conf:/opt/zeppelin/conf
      - ./data:/usr/data:rw
      - spark_volume:/opt/zeppelin/spark
    environment:
      - "SPARK_HOME=/opt/zeppelin/spark"
      - "SPARK_MASTER=spark://spark-master:7077"
      - "PYTHONPATH=/opt/zeppelin/spark/python"
    # command: /bin/bash -c "su root && \
    #           -w /opt/zeppelin && \
    #           cp -a ./jars/* ./spark/jars/ && \
    #           rm -r ./jars && \
    #           su 1000 && \
    #           /opt/zeppelin/bin/zeppelin.sh"
    # stdin_open: true
    # tty: true
volumes:
    spark_volume: